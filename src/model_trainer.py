import os
import json
import numpy as np
from datetime import datetime
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix


def build_mobilenetv2(num_classes):
    # (Gi·ªØ nguy√™n code c·ªßa b·∫°n ·ªü ƒëo·∫°n n√†y)
    base_model = MobileNetV2(
        weights="imagenet",
        include_top=False,
        input_shape=(224, 224, 3)
    )
    base_model.trainable = False
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.3)(x)
    x = Dense(128, activation="relu")(x)
    x = Dropout(0.2)(x)
    outputs = Dense(num_classes, activation="softmax")(x)
   
    model = Model(inputs=base_model.input, outputs=outputs)
   
    model.compile(
        optimizer=Adam(learning_rate=0.0005),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model


def train_model(model, train_gen, val_gen, epochs=20, class_weights=None):
    # T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥
    if not os.path.exists("models"):
        os.makedirs("models")
   
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            "models/MobileNetV2_best.h5",
            save_best_only=True,
            monitor="val_accuracy",
            mode="max",
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            patience=5,
            restore_best_weights=True,
            monitor="val_loss",
            verbose=1
        )
    ]

    # Train v·ªõi class weights n·∫øu c√≥
    fit_params = {
        "x": train_gen,
        "validation_data": val_gen,
        "epochs": epochs,
        "callbacks": callbacks
    }
    
    if class_weights:
        fit_params["class_weight"] = class_weights
        print(f"   [INFO] ƒêang s·ª≠ d·ª•ng class weights ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu")

    history = model.fit(**fit_params)

    return history


def evaluate_and_save_report(model, test_gen, class_names, output_dir="models"):
    print("\n" + "=" * 60)
    print("ƒêANG ƒê√ÅNH GI√Å M√î H√åNH TR√äN TEST SET...")
    print("=" * 60)
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # Predict tr√™n test set
    print("ƒêang d·ª± ƒëo√°n tr√™n test set...")
    y_true = []
    y_pred = []
    
    # Reset generator
    test_gen.reset()
    steps = len(test_gen)
    
    for step in range(steps):
        batch_x, batch_y = next(test_gen)
        predictions = model.predict(batch_x, verbose=0)
        
        # L·∫•y true labels v√† predictions
        batch_y_true = np.argmax(batch_y, axis=1)
        batch_y_pred = np.argmax(predictions, axis=1)
        
        y_true.extend(batch_y_true)
        y_pred.extend(batch_y_pred)
        
        if (step + 1) % 10 == 0:
            print(f"  ƒê√£ x·ª≠ l√Ω {step + 1}/{steps} batches...")
    
    # Convert to numpy array
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # T√≠nh to√°n metrics
    print("\nƒêang t√≠nh to√°n metrics...")
    report_dict = classification_report(
        y_true, y_pred,
        target_names=class_names,
        output_dict=True,
        zero_division=0
    )
    
    # T√≠nh confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # L∆∞u classification report
    report_file = os.path.join(output_dir, "evaluation_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_dict, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ ƒê√£ l∆∞u evaluation report v√†o: {report_file}")
    
    # In t√≥m t·∫Øt
    print("\nüìä T√ìM T·∫ÆT ƒê√ÅNH GI√Å:")
    print(f"   - Accuracy: {report_dict['accuracy']:.4f}")
    print(f"   - Macro Avg F1: {report_dict['macro avg']['f1-score']:.4f}")
    print(f"   - Weighted Avg F1: {report_dict['weighted avg']['f1-score']:.4f}")
    
    # In top 5 classes t·ªët nh·∫•t v√† k√©m nh·∫•t
    class_f1 = {}
    for class_name in class_names:
        if class_name in report_dict:
            class_f1[class_name] = report_dict[class_name]['f1-score']
    
    sorted_classes = sorted(class_f1.items(), key=lambda x: x[1], reverse=True)
    
    print("\nüèÜ TOP 5 CLASSES T·ªêT NH·∫§T:")
    for i, (class_name, f1) in enumerate(sorted_classes[:5], 1):
        print(f"   {i}. {class_name}: F1={f1:.4f}")
    
    print("\n‚ö†Ô∏è  TOP 5 CLASSES K√âM NH·∫§T:")
    for i, (class_name, f1) in enumerate(sorted_classes[-5:], 1):
        print(f"   {i}. {class_name}: F1={f1:.4f}")
    
    return report_dict, cm